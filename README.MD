# Proyecto de Scraping Web y API REST

Un sistema completo de web scraping con FastAPI que extrae informaci√≥n de productos de sitios web y los expone a trav√©s de una API REST con capacidades de filtrado y gesti√≥n.

## Caracter√≠sticas

- üîç **Web scraping robusto** con BeautifulSoup que maneja errores y reintentos
- üíæ **Almacenamiento en base de datos SQL** con SQLAlchemy
- üåê **API REST completa** con FastAPI que incluye:
  - Filtros avanzados (precio, t√≠tulo, categor√≠a, rating)
  - Paginaci√≥n integrada
  - Documentaci√≥n autom√°tica con Swagger UI
- üöÄ **Endpoint para iniciar scraping** desde la API
- üóëÔ∏è **Gesti√≥n de datos** con capacidad para limpiar la base de datos
- üìä **Estad√≠sticas** de productos recopilados
- üê≥ **Dockerizado** para f√°cil despliegue local y en la nube
- ‚òÅÔ∏è **Listo para Render.com** para despliegue r√°pido

## Estructura del Proyecto

```
scraping-api-project/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # Archivo vac√≠o para marcar el directorio como paquete
‚îÇ   ‚îú‚îÄ‚îÄ models.py         # Modelos de la base de datos y Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ database.py       # Configuraci√≥n de la base de datos
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py        # L√≥gica del web scraping
‚îÇ   ‚îú‚îÄ‚îÄ api.py            # Endpoints de la API
‚îÇ   ‚îî‚îÄ‚îÄ main.py           # Punto de entrada principal
‚îú‚îÄ‚îÄ requirements.txt      # Dependencias
‚îú‚îÄ‚îÄ Dockerfile            # Configuraci√≥n de Docker
‚îú‚îÄ‚îÄ docker-compose.yml    # Configuraci√≥n de Docker Compose
‚îú‚îÄ‚îÄ .env                  # Variables de entorno
‚îî‚îÄ‚îÄ README.md             # Documentaci√≥n
```

## Requisitos previos

- Python 3.9+
- Docker y Docker Compose (opcional, para despliegue en contenedores)
- Git (para clonar el repositorio)

## Instalaci√≥n y Ejecuci√≥n Local

### Opci√≥n 1: Con Python directamente

1. **Clonar el repositorio:**
   ```bash
   git clone 
   ```

2. **Crear y activar entorno virtual:**
   ```bash
   python -m venv venv
   
   # En Windows:
   venv\Scripts\activate
   
   # En macOS/Linux:
   source venv/bin/activate
   ```

3. **Instalar dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Ejecutar la aplicaci√≥n:**
   ```bash
   python -m app.main
   ```

5. **Acceder a la API:**
   - Documentaci√≥n: http://127.0.0.1:8000/docs
   - API: http://127.0.0.1:8000

### Opci√≥n 2: Con Docker

1. **Clonar el repositorio:**
   ```bash
   git clone 
   ```

2. **Construir y ejecutar con Docker Compose:**
   ```bash
   docker-compose up --build
   ```

3. **Acceder a la API:**
   - Documentaci√≥n: http://localhost:8001/docs
   - API: http://localhost:8001

## Uso de la API

Una vez que la aplicaci√≥n est√° en ejecuci√≥n, puedes usar la API de las siguientes formas:

### 1. Iniciar el proceso de scraping

Puedes iniciar el scraping directamente desde la API:

```bash
curl -X POST "http://localhost:8001/scrape/?url=https://books.toscrape.com&pages=5"
```

O desde la interfaz web de Swagger UI en `/docs`.

### 2. Consultar productos

Obtener todos los productos:
```bash
curl "http://localhost:8001/products/"
```

Con filtros:
```bash
curl "http://localhost:8001/products/?min_price=20&category=fiction&min_rating=4"
```

### 3. Eliminar todos los productos

Para borrar todos los productos y empezar de nuevo:
```bash
curl -X DELETE "http://localhost:8001/products/"
```

### 4. Ver estad√≠sticas

Para obtener estad√≠sticas de los productos recopilados:
```bash
curl "http://localhost:8001/stats/"
```

## Endpoints principales

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| GET | `/products/` | Obtiene productos con filtros opcionales |
| GET | `/products/{id}` | Obtiene un producto espec√≠fico por ID |
| POST | `/scrape/` | Inicia el scraping con par√°metros personalizados |
| DELETE | `/products/` | Elimina todos los productos de la base de datos |
| GET | `/stats/` | Obtiene estad√≠sticas sobre los productos recopilados |
| GET | `/categories/` | Obtiene todas las categor√≠as disponibles |
| GET | `/health` | Health check (√∫til para monitoreo) |

## Par√°metros de filtrado

En el endpoint `/products/` puedes utilizar los siguientes par√°metros:

- `min_price`: Precio m√≠nimo 
- `max_price`: Precio m√°ximo
- `name`: B√∫squeda por texto en el t√≠tulo
- `category`: Filtro por categor√≠a exacta
- `min_rating`: Rating m√≠nimo (0-5)
- `skip`: Para paginaci√≥n, n√∫mero de resultados a omitir
- `limit`: N√∫mero m√°ximo de resultados a devolver

## Despliegue en Render.com

Para desplegar la aplicaci√≥n en Render.com, sigue estos pasos:

1. **Sube tu c√≥digo a un repositorio GitHub**

2. **Crea un nuevo Web Service en Render:**
   - Inicia sesi√≥n en [Render](https://render.com)
   - Haz clic en "New +"
   - Selecciona "Web Service"
   - Conecta tu repositorio de GitHub
   - Selecciona el repositorio con tu proyecto

3. **Configura el servicio:**
   - **Name**: Un nombre para tu servicio (ej. "scraping-api")
   - **Region**: Elige la regi√≥n m√°s cercana a tus usuarios
   - **Branch**: main (o master)
   - **Runtime**: Docker
   - **Instance Type**: Free (o el plan que necesites)

4. **Configura variables de entorno:**
   - `DATABASE_URL`: Para una base de datos persistente, usa PostgreSQL en Render
   - `HOST`: 0.0.0.0
   - `PORT`: 10000

5. **Health Check (opcional):**
   - **Path**: `/health`

6. **Crea el servicio:**
   - Haz clic en "Create Web Service"

7. **Ejecuta el scraper (opcional):**
   - Una vez que el servicio est√© en funcionamiento, puedes usar el endpoint `/scrape/` o bien:
   - Ve a la pesta√±a "Shell" en el dashboard de Render
   - Ejecuta: `python -m app.main --scrape --url https://books.toscrape.com --pages 5`

8. **Accede a tu API:**
   - La URL ser√° algo como `https://tu-servicio.onrender.com`
   - La documentaci√≥n estar√° en `https://tu-servicio.onrender.com/docs`

## Personalizaci√≥n del scraper

Para adaptar el scraper a otro sitio web:

1. Modifica la funci√≥n `parse_product_list` en `scraper.py`.
2. Ajusta los selectores CSS seg√∫n la estructura de la p√°gina objetivo.
3. Si es necesario, modifica el modelo `ProductDB` para almacenar datos adicionales.

## Soluci√≥n de problemas

### Error "address already in use"
Si obtienes un error de puerto en uso, modifica el puerto en `docker-compose.yml`:
```yaml
ports:
  - "8002:8000"  # Cambia 8001 por otro puerto disponible
```

### Problemas de codificaci√≥n al extraer datos
El c√≥digo ya maneja caracteres especiales como `√Ç` en precios, pero si encuentras otros problemas, revisa la funci√≥n `parse_product_list` en `scraper.py`.

## Consideraciones para producci√≥n

1. **Seguridad**: A√±ade autenticaci√≥n a tu API (como JWT)
2. **Base de datos**: Cambia SQLite por PostgreSQL
3. **CORS**: Configura adecuadamente los dominios permitidos
4. **Rate limiting**: A√±ade limitaciones de tasa para prevenir abusos
5. **Monitoreo**: Implementa herramientas de monitoreo y alertas
6. **Backups**: Configura copias de seguridad de la base de datos